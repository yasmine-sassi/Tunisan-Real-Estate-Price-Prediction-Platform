{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Improved Real Estate Price Prediction Preprocessing\n",
    "## Tunisian Real Estate Dataset\n",
    "\n",
    "**Key Improvements:**\n",
    "1. Separate pipelines for rent and sale transactions\n",
    "2. Better handling of high-cardinality features (region)\n",
    "3. Feature engineering (price per sqm, region aggregations)\n",
    "4. Proper treatment of ordinal features\n",
    "5. Rare category grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\mediatek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.24.1 in c:\\users\\mediatek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (2.2.3)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\mediatek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\mediatek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\mediatek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data_header",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data at: C:\\Users\\Mediatek\\Desktop\\ML-project\\ML\\data\\processed\\cleaned_real_estate.csv\n",
      "Total records: 9,744\n",
      "\n",
      "Transaction distribution:\n",
      "transaction\n",
      "rent    5636\n",
      "sale    4108\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Rent: 5,636 (57.8%)\n",
      "Sale: 4,108 (42.2%)\n"
     ]
    }
   ],
   "source": [
    "# Resolve data path from workspace\n",
    "def find_data_path():\n",
    "    candidates = [\n",
    "        Path.cwd() / \"data\" / \"processed\" / \"cleaned_real_estate.csv\",\n",
    "        Path.cwd().parent / \"data\" / \"processed\" / \"cleaned_real_estate.csv\",\n",
    "        Path.cwd().parent / \"ML\" / \"data\" / \"processed\" / \"cleaned_real_estate.csv\",\n",
    "        Path.home() / \"Desktop\" / \"ML-project\" / \"ML\" / \"data\" / \"processed\" / \"cleaned_real_estate.csv\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            print(f\"Found data at: {p}\")\n",
    "            return p\n",
    "    print(\"Checked paths:\")\n",
    "    for p in candidates:\n",
    "        print(f\"  - {p}\")\n",
    "    raise FileNotFoundError(\"cleaned_real_estate.csv not found in expected locations\")\n",
    "\n",
    "DATA_PATH = find_data_path()\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"\\nTransaction distribution:\")\n",
    "print(df['transaction'].value_counts())\n",
    "print(f\"\\nRent: {len(df[df['transaction']=='rent']):,} ({len(df[df['transaction']=='rent'])/len(df)*100:.1f}%)\")\n",
    "print(f\"Sale: {len(df[df['transaction']=='sale']):,} ({len(df[df['transaction']=='sale'])/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_eng_header",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "Creating new features that will help the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "feature_engineering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original regions: 267\n",
      "Grouped regions: 100\n",
      "Rare regions grouped: 168\n"
     ]
    }
   ],
   "source": [
    "# Basic cleaning\n",
    "required_cols = ['price', 'surface', 'rooms', 'bathrooms', 'region', 'city', 'property_type', 'transaction']\n",
    "df = df.dropna(subset=required_cols).copy()\n",
    "df = df[(df['price'] > 0) & (df['surface'] > 0)]\n",
    "\n",
    "def cap_outliers_iqr(series, k=1.5):\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - k * iqr\n",
    "    upper = q3 + k * iqr\n",
    "    return series.clip(lower, upper)\n",
    "\n",
    "# Cap extreme outliers to stabilize training\n",
    "for col in ['price', 'surface', 'rooms', 'bathrooms']:\n",
    "    df[col] = cap_outliers_iqr(df[col])\n",
    "\n",
    "# Feature 1: Total rooms (rooms + bathrooms)\n",
    "df['total_rooms'] = df['rooms'] + df['bathrooms']\n",
    "\n",
    "# Feature 2: Surface category (small, medium, large)\n",
    "df['surface_category'] = pd.cut(df['surface'],\n",
    "                                  bins=[0, 80, 150, 300, 10000],\n",
    "                                  labels=['small', 'medium', 'large', 'very_large'])\n",
    "\n",
    "# Feature 3: Group rare regions (< 20 occurrences) into 'other'\n",
    "# The REGION itself matters (e.g., \"Lac\" is expensive, \"Hay Tahrir\" is cheaper)\n",
    "# We'll use target encoding to convert region names to their mean prices\n",
    "region_counts = df['region'].value_counts()\n",
    "df['region_grouped'] = df['region'].copy()\n",
    "rare_regions = region_counts[region_counts < 20].index\n",
    "df.loc[df['region'].isin(rare_regions), 'region_grouped'] = 'other_region'\n",
    "\n",
    "# Feature 4: Log transforms for skewed variables\n",
    "df['log_surface'] = np.log1p(df['surface'])\n",
    "df['log_total_rooms'] = np.log1p(df['total_rooms'])\n",
    "\n",
    "print(f\"Original regions: {df['region'].nunique()}\")\n",
    "print(f\"Grouped regions: {df['region_grouped'].nunique()}\")\n",
    "print(f\"Rare regions grouped: {len(rare_regions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "target_encoding_header",
   "metadata": {},
   "source": [
    "## 3. Target Encoding for High Cardinality Features\n",
    "\n",
    "For regions, we'll create mean-encoded features (average price by region):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "target_encoding_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_encoding(df, column, target, min_samples=10, smoothing=10):\n",
    "    \"\"\"\n",
    "    Create target encoding with smoothing for rare categories.\n",
    "    Returns the mapping dictionary and global mean.\n",
    "    \"\"\"\n",
    "    global_mean = df[target].mean()\n",
    "    agg = df.groupby(column)[target].agg(['mean', 'count'])\n",
    "    agg['count'] = agg['count'].clip(lower=min_samples)\n",
    "    agg['smoothed_mean'] = (\n",
    "        (agg['count'] * agg['mean'] + smoothing * global_mean) /\n",
    "        (agg['count'] + smoothing)\n",
    "    )\n",
    "    return agg['smoothed_mean'].to_dict(), global_mean\n",
    "\n",
    "def add_kfold_target_encoding(X, y, column, n_splits=5, min_samples=10, smoothing=10):\n",
    "    \"\"\"\n",
    "    Out-of-fold target encoding for training data, plus full mapping for test.\n",
    "    \"\"\"\n",
    "    oof_encoded = pd.Series(index=X.index, dtype=float)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        train_fold = pd.concat([X.iloc[train_idx], y.iloc[train_idx]], axis=1)\n",
    "        mapping, global_mean = create_target_encoding(\n",
    "            train_fold, column, y.name, min_samples=min_samples, smoothing=smoothing\n",
    "        )\n",
    "        oof_encoded.iloc[val_idx] = X.iloc[val_idx][column].map(mapping).fillna(global_mean)\n",
    "    full_mapping, full_global = create_target_encoding(\n",
    "        pd.concat([X, y], axis=1), column, y.name, min_samples=min_samples, smoothing=smoothing\n",
    "    )\n",
    "    return oof_encoded, full_mapping, full_global"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split_header",
   "metadata": {},
   "source": [
    "## 4. Split Data by Transaction Type\n",
    "\n",
    "**Critical Step:** Separate rent and sale data for independent modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "split_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rent dataset: 5,636 records\n",
      "Sale dataset: 4,108 records\n"
     ]
    }
   ],
   "source": [
    "# Split by transaction type\n",
    "df_rent = df[df['transaction'] == 'rent'].copy()\n",
    "df_sale = df[df['transaction'] == 'sale'].copy()\n",
    "\n",
    "print(f\"Rent dataset: {len(df_rent):,} records\")\n",
    "print(f\"Sale dataset: {len(df_sale):,} records\")\n",
    "\n",
    "# Drop transaction column as it's no longer needed\n",
    "df_rent = df_rent.drop(columns=['transaction'])\n",
    "df_sale = df_sale.drop(columns=['transaction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "target_encoding_apply_header",
   "metadata": {},
   "source": [
    "## 5. Create Target Encodings (Separate for Rent and Sale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "create_encodings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 10\n",
      "Categorical: ['property_type', 'city', 'surface_category']\n",
      "Numerical: ['surface', 'log_surface', 'log_total_rooms']\n",
      "Ordinal: ['rooms', 'bathrooms', 'total_rooms']\n",
      "Target encoded (REGION): ['region_grouped']\n"
     ]
    }
   ],
   "source": [
    "# We'll create these encodings on the training set only (to avoid data leakage)\n",
    "TARGET = 'price'\n",
    "\n",
    "# Define feature groups\n",
    "CATEGORICAL_COLS = [\n",
    "    'property_type',\n",
    "    'city',\n",
    "    'surface_category'\n",
    "]\n",
    "\n",
    "NUMERICAL_COLS = [\n",
    "    'surface',\n",
    "    'log_surface',\n",
    "    'log_total_rooms'\n",
    "]\n",
    "\n",
    "# Treat rooms and bathrooms as ordinal (order matters)\n",
    "ORDINAL_COLS = [\n",
    "    'rooms',\n",
    "    'bathrooms',\n",
    "    'total_rooms'\n",
    "]\n",
    "\n",
    "# High cardinality features for target encoding\n",
    "# REGION will be target-encoded to its mean price (most important feature!)\n",
    "TARGET_ENCODE_COLS = [\n",
    "    'region_grouped'\n",
    "]\n",
    "\n",
    "ALL_FEATURES = CATEGORICAL_COLS + NUMERICAL_COLS + ORDINAL_COLS + TARGET_ENCODE_COLS\n",
    "\n",
    "print(f\"Total features: {len(ALL_FEATURES)}\")\n",
    "print(f\"Categorical: {CATEGORICAL_COLS}\")\n",
    "print(f\"Numerical: {NUMERICAL_COLS}\")\n",
    "print(f\"Ordinal: {ORDINAL_COLS}\")\n",
    "print(f\"Target encoded (REGION): {TARGET_ENCODE_COLS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_test_split_header",
   "metadata": {},
   "source": [
    "## 6. Train-Test Split (Separate for Each Transaction Type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "train_test_split",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RENT SPLIT:\n",
      "  Train: 4,508 samples\n",
      "  Test:  1,128 samples\n",
      "\n",
      "SALE SPLIT:\n",
      "  Train: 3,286 samples\n",
      "  Test:  822 samples\n"
     ]
    }
   ],
   "source": [
    "def stratified_split(X, y, test_size=0.2, random_state=42, n_bins=10):\n",
    "    try:\n",
    "        y_binned = pd.qcut(y, q=n_bins, duplicates='drop')\n",
    "        return train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=y_binned\n",
    "        )\n",
    "    except ValueError:\n",
    "        return train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "# RENT data split\n",
    "X_rent = df_rent[ALL_FEATURES]\n",
    "y_rent = df_rent[TARGET]\n",
    "\n",
    "X_rent_train, X_rent_test, y_rent_train, y_rent_test = stratified_split(\n",
    "    X_rent, y_rent, test_size=0.2, random_state=42, n_bins=10\n",
    " )\n",
    "\n",
    "print(\"RENT SPLIT:\")\n",
    "print(f\"  Train: {len(X_rent_train):,} samples\")\n",
    "print(f\"  Test:  {len(X_rent_test):,} samples\")\n",
    "\n",
    "# SALE data split\n",
    "X_sale = df_sale[ALL_FEATURES]\n",
    "y_sale = df_sale[TARGET]\n",
    "\n",
    "X_sale_train, X_sale_test, y_sale_train, y_sale_test = stratified_split(\n",
    "    X_sale, y_sale, test_size=0.2, random_state=42, n_bins=10\n",
    " )\n",
    "\n",
    "print(\"\\nSALE SPLIT:\")\n",
    "print(f\"  Train: {len(X_sale_train):,} samples\")\n",
    "print(f\"  Test:  {len(X_sale_test):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "target_encoding_fit_header",
   "metadata": {},
   "source": [
    "## 7. Fit Target Encodings on Training Data Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fit_target_encodings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target encodings created successfully!\n",
      "\n",
      "Rent - example region encodings:\n",
      "  Ain Zaghouan Nord: 1542.03 TND\n",
      "  Ain Zaghouan Sud: 1300.10 TND\n",
      "  Ain Zaghouen: 1274.61 TND\n",
      "  Akouda: 1283.64 TND\n",
      "  Ariana: 1121.10 TND\n",
      "\n",
      "Sale - example region encodings:\n",
      "  Ain Zaghouan Nord: 394765.71 TND\n",
      "  Ain Zaghouan Sud: 487365.08 TND\n",
      "  Ain Zaghouen: 317732.81 TND\n",
      "  Akouda: 469138.89 TND\n",
      "  Ariana: 413266.67 TND\n"
     ]
    }
   ],
   "source": [
    "# Create target encodings for RENT (out-of-fold)\n",
    "rent_region_oof, rent_region_mapping, rent_region_global = add_kfold_target_encoding(\n",
    "    X_rent_train, y_rent_train, 'region_grouped', n_splits=5, min_samples=10, smoothing=10\n",
    " )\n",
    "\n",
    "# Create target encodings for SALE (out-of-fold)\n",
    "sale_region_oof, sale_region_mapping, sale_region_global = add_kfold_target_encoding(\n",
    "    X_sale_train, y_sale_train, 'region_grouped', n_splits=5, min_samples=10, smoothing=10\n",
    " )\n",
    "\n",
    "print(\"Target encodings created successfully!\")\n",
    "print(f\"\\nRent - example region encodings:\")\n",
    "for region, value in list(rent_region_mapping.items())[:5]:\n",
    "    print(f\"  {region}: {value:.2f} TND\")\n",
    "    \n",
    "print(f\"\\nSale - example region encodings:\")\n",
    "for region, value in list(sale_region_mapping.items())[:5]:\n",
    "    print(f\"  {region}: {value:.2f} TND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apply_target_encoding_header",
   "metadata": {},
   "source": [
    "## 8. Apply Target Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply_target_encodings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target encoding + advanced feature engineering applied!\n",
      "\n",
      "New features added: surface_x_rooms, surface_x_region, rooms_x_bathrooms, surface_squared\n",
      "Total numerical columns: 8\n"
     ]
    }
   ],
   "source": [
    "# Apply to RENT data\n",
    "X_rent_train = X_rent_train.copy()\n",
    "X_rent_test = X_rent_test.copy()\n",
    "X_rent_train['region_encoded'] = rent_region_oof\n",
    "X_rent_test['region_encoded'] = X_rent_test['region_grouped'].map(rent_region_mapping).fillna(rent_region_global)\n",
    "\n",
    "# Apply to SALE data\n",
    "X_sale_train = X_sale_train.copy()\n",
    "X_sale_test = X_sale_test.copy()\n",
    "X_sale_train['region_encoded'] = sale_region_oof\n",
    "X_sale_test['region_encoded'] = X_sale_test['region_grouped'].map(sale_region_mapping).fillna(sale_region_global)\n",
    "\n",
    "# Drop the original region column\n",
    "X_rent_train = X_rent_train.drop(columns=['region_grouped'])\n",
    "X_rent_test = X_rent_test.drop(columns=['region_grouped'])\n",
    "X_sale_train = X_sale_train.drop(columns=['region_grouped'])\n",
    "X_sale_test = X_sale_test.drop(columns=['region_grouped'])\n",
    "\n",
    "# Update feature lists - add region_encoded to numerical\n",
    "NUMERICAL_COLS.append('region_encoded')\n",
    "print(\"Target encoding applied!\")\n",
    "print(f\"\\nUpdated numerical columns: {NUMERICAL_COLS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing_pipeline_header",
   "metadata": {},
   "source": [
    "## 9. Create Preprocessing Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "create_preprocessors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing pipelines created!\n"
     ]
    }
   ],
   "source": [
    "# Create the same preprocessor for both rent and sale\n",
    "def create_preprocessor():\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\n",
    "                'categorical',\n",
    "                OneHotEncoder(handle_unknown='ignore', sparse_output=False),\n",
    "                CATEGORICAL_COLS\n",
    "            ),\n",
    "            (\n",
    "                'numerical',\n",
    "                StandardScaler(),\n",
    "                NUMERICAL_COLS\n",
    "            ),\n",
    "            (\n",
    "                'ordinal',\n",
    "                StandardScaler(),  # Or keep as-is, depending on your preference\n",
    "                ORDINAL_COLS\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "preprocessor_rent = create_preprocessor()\n",
    "preprocessor_sale = create_preprocessor()\n",
    "\n",
    "print(\"Preprocessing pipelines created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fit_transform_header",
   "metadata": {},
   "source": [
    "## 10. Fit and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fit_transform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RENT data preprocessed:\n",
      "  Train shape: (4508, 42)\n",
      "  Test shape:  (1128, 42)\n",
      "\n",
      "SALE data preprocessed:\n",
      "  Train shape: (3286, 42)\n",
      "  Test shape:  (822, 42)\n",
      "\n",
      "âœ… Feature shapes verified - no mismatches!\n"
     ]
    }
   ],
   "source": [
    "# RENT preprocessing\n",
    "X_rent_train_prepared = preprocessor_rent.fit_transform(X_rent_train)\n",
    "X_rent_test_prepared = preprocessor_rent.transform(X_rent_test)\n",
    "\n",
    "print(\"RENT data preprocessed:\")\n",
    "print(f\"  Train shape: {X_rent_train_prepared.shape}\")\n",
    "print(f\"  Test shape:  {X_rent_test_prepared.shape}\")\n",
    "\n",
    "# SALE preprocessing\n",
    "X_sale_train_prepared = preprocessor_sale.fit_transform(X_sale_train)\n",
    "X_sale_test_prepared = preprocessor_sale.transform(X_sale_test)\n",
    "\n",
    "print(\"\\nSALE data preprocessed:\")\n",
    "print(f\"  Train shape: {X_sale_train_prepared.shape}\")\n",
    "print(f\"  Test shape:  {X_sale_test_prepared.shape}\")\n",
    "\n",
    "# Verify shapes match for train/test pairs\n",
    "assert X_rent_train_prepared.shape[1] == X_rent_test_prepared.shape[1], \"RENT: Train/Test feature mismatch!\"\n",
    "assert X_sale_train_prepared.shape[1] == X_sale_test_prepared.shape[1], \"SALE: Train/Test feature mismatch!\"\n",
    "print(\"\\nâœ… Feature shapes verified - no mismatches!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_header",
   "metadata": {},
   "source": [
    "## 11. Save Preprocessed Data and Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All data and pipelines saved successfully!\n",
      "\n",
      "Output directory: C:\\Users\\Mediatek\n",
      "\n",
      "Files saved:\n",
      "  - Rent: X_train, X_test, y_train, y_test\n",
      "  - Sale: X_train, X_test, y_train, y_test\n",
      "  - Preprocessors: preprocessor_rent.joblib, preprocessor_sale.joblib\n",
      "  - Target encodings: target_encodings_rent.joblib, target_encodings_sale.joblib\n"
     ]
    }
   ],
   "source": [
    "# Create output directories - explicitly under ML/data/\n",
    "PREP_VERSION = \"v2\"\n",
    "\n",
    "# Direct path to ML directory\n",
    "ML_DIR = Path(\"C:/Users/Mediatek/Desktop/ML-project/ML\")\n",
    "\n",
    "# Verify the directory exists\n",
    "if not ML_DIR.exists():\n",
    "    print(f\"âš ï¸  Path does not exist: {ML_DIR}\")\n",
    "    print(\"Trying alternate paths...\")\n",
    "    candidates = [\n",
    "        Path.cwd(),\n",
    "        Path.cwd().parent,\n",
    "        Path.cwd().parent / \"ML\",\n",
    "        Path.home() / \"Desktop\" / \"ML-project\" / \"ML\"\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if (p / \"data\" / \"processed\" / \"cleaned_real_estate.csv\").exists():\n",
    "            ML_DIR = p\n",
    "            break\n",
    "\n",
    "print(f\"âœ… ML directory: {ML_DIR}\")\n",
    "OUTPUT_DIR = ML_DIR\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(OUTPUT_DIR / f\"data/prepared_{PREP_VERSION}/rent\", exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR / f\"data/prepared_{PREP_VERSION}/sale\", exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR / f\"data/preprocessing_{PREP_VERSION}\", exist_ok=True)\n",
    "\n",
    "print(f\"\\nðŸ“ Output paths:\")\n",
    "print(f\"  prepared_{PREP_VERSION}: {OUTPUT_DIR / f'data/prepared_{PREP_VERSION}'}\")\n",
    "print(f\"  preprocessing_{PREP_VERSION}: {OUTPUT_DIR / f'data/preprocessing_{PREP_VERSION}'}\")\n",
    "\n",
    "# Save RENT data\n",
    "np.save(OUTPUT_DIR / f\"data/prepared_{PREP_VERSION}/rent/X_train.npy\", X_rent_train_prepared)\n",
    "np.save(OUTPUT_DIR / f\"data/prepared_{PREP_VERSION}/rent/X_test.npy\", X_rent_test_prepared)\n",
    "np.save(OUTPUT_DIR / f\"data/prepared_{PREP_VERSION}/rent/y_train.npy\", y_rent_train.to_numpy())\n",
    "np.save(OUTPUT_DIR / f\"data/prepared_{PREP_VERSION}/rent/y_test.npy\", y_rent_test.to_numpy())\n",
    "\n",
    "# Save SALE data\n",
    "np.save(OUTPUT_DIR / f\"data/prepared_{PREP_VERSION}/sale/X_train.npy\", X_sale_train_prepared)\n",
    "np.save(OUTPUT_DIR / f\"data/prepared_{PREP_VERSION}/sale/X_test.npy\", X_sale_test_prepared)\n",
    "np.save(OUTPUT_DIR / f\"data/prepared_{PREP_VERSION}/sale/y_train.npy\", y_sale_train.to_numpy())\n",
    "np.save(OUTPUT_DIR / f\"data/prepared_{PREP_VERSION}/sale/y_test.npy\", y_sale_test.to_numpy())\n",
    "\n",
    "# Save preprocessors\n",
    "joblib.dump(\n",
    "    preprocessor_rent,\n",
    "    OUTPUT_DIR / f\"data/preprocessing_{PREP_VERSION}/preprocessor_rent.joblib\"\n",
    " )\n",
    "joblib.dump(\n",
    "    preprocessor_sale,\n",
    "    OUTPUT_DIR / f\"data/preprocessing_{PREP_VERSION}/preprocessor_sale.joblib\"\n",
    " )\n",
    "\n",
    "# Save target encodings\n",
    "joblib.dump(\n",
    "    {'region_encoding': rent_region_mapping, 'region_global': rent_region_global},\n",
    "    OUTPUT_DIR / f\"data/preprocessing_{PREP_VERSION}/target_encodings_rent.joblib\"\n",
    " )\n",
    "joblib.dump(\n",
    "    {'region_encoding': sale_region_mapping, 'region_global': sale_region_global},\n",
    "    OUTPUT_DIR / f\"data/preprocessing_{PREP_VERSION}/target_encodings_sale.joblib\"\n",
    " )\n",
    "\n",
    "print(\"\\nâœ… All data and pipelines saved successfully!\")\n",
    "print(f\"\\nFiles saved:\")\n",
    "print(f\"  Rent prepared: {OUTPUT_DIR / f'data/prepared_{PREP_VERSION}/rent/'}\")\n",
    "print(f\"  Sale prepared: {OUTPUT_DIR / f'data/prepared_{PREP_VERSION}/sale/'}\")\n",
    "print(f\"  Preprocessing: {OUTPUT_DIR / f'data/preprocessing_{PREP_VERSION}/'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "## 12. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREPROCESSING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š DATASET SIZES:\n",
      "  Rent Train:  4,508 samples x 42 features\n",
      "  Rent Test:   1,128 samples x 42 features\n",
      "  Sale Train:  3,286 samples x 42 features\n",
      "  Sale Test:   822 samples x 42 features\n",
      "\n",
      "ðŸ’° PRICE STATISTICS:\n",
      "  RENT:\n",
      "    Train mean: 1458.95 TND (std: 1367.79)\n",
      "    Test mean:  1499.33 TND (std: 1486.87)\n",
      "    Price range: 200 - 15000 TND\n",
      "  SALE:\n",
      "    Train mean: 406366.68 TND (std: 210805.66)\n",
      "    Test mean:  406126.73 TND (std: 211631.32)\n",
      "    Price range: 20000 - 748500 TND\n",
      "\n",
      "ðŸ”§ FEATURE ENGINEERING:\n",
      "  Base features: 7\n",
      "  After feature engineering: 10\n",
      "  After one-hot encoding: 42 features (rent)\n",
      "  Advanced features: surface_x_rooms, surface_x_region, rooms_x_bathrooms, surface_squared\n",
      "\n",
      "âš ï¸  NOTE:\n",
      "  RÂ² scores of 0.55-0.62 indicate:\n",
      "  - Missing important features (condition, amenities, location proximity)\n",
      "  - High inherent price variance in the data\n",
      "  - Consider collecting more detailed property attributes\n",
      "\n",
      "âœ… Ready for model training with enhanced features!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nðŸ“Š DATASET SIZES:\")\n",
    "print(f\"  Rent Train:  {X_rent_train_prepared.shape[0]:,} samples x {X_rent_train_prepared.shape[1]} features\")\n",
    "print(f\"  Rent Test:   {X_rent_test_prepared.shape[0]:,} samples x {X_rent_test_prepared.shape[1]} features\")\n",
    "print(f\"  Sale Train:  {X_sale_train_prepared.shape[0]:,} samples x {X_sale_train_prepared.shape[1]} features\")\n",
    "print(f\"  Sale Test:   {X_sale_test_prepared.shape[0]:,} samples x {X_sale_test_prepared.shape[1]} features\")\n",
    "\n",
    "print(\"\\nðŸ’° PRICE STATISTICS:\")\n",
    "print(\"  RENT:\")\n",
    "print(f\"    Train mean: {y_rent_train.mean():.2f} TND (std: {y_rent_train.std():.2f})\")\n",
    "print(f\"    Test mean:  {y_rent_test.mean():.2f} TND (std: {y_rent_test.std():.2f})\")\n",
    "print(f\"    Price range: {y_rent_train.min():.0f} - {y_rent_train.max():.0f} TND\")\n",
    "print(\"  SALE:\")\n",
    "print(f\"    Train mean: {y_sale_train.mean():.2f} TND (std: {y_sale_train.std():.2f})\")\n",
    "print(f\"    Test mean:  {y_sale_test.mean():.2f} TND (std: {y_sale_test.std():.2f})\")\n",
    "print(f\"    Price range: {y_sale_train.min():.0f} - {y_sale_train.max():.0f} TND\")\n",
    "\n",
    "print(\"\\nðŸ”§ FEATURE ENGINEERING:\")\n",
    "print(f\"  Original features: 7\")\n",
    "print(f\"  Engineered features: {len(ALL_FEATURES) - 7}\")\n",
    "print(f\"  Total before encoding: {len(ALL_FEATURES)}\")\n",
    "print(f\"  Total after one-hot: {X_rent_train_prepared.shape[1]}\")\n",
    "\n",
    "print(\"\\nâœ… Ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps_header",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now you're ready to train separate models:\n",
    "\n",
    "1. **For RENT model:**\n",
    "   - Load: `X_rent_train.npy`, `y_rent_train.npy`\n",
    "   - Train models (Linear Regression, Random Forest, XGBoost, etc.)\n",
    "   - Evaluate on `X_rent_test.npy`, `y_rent_test.npy`\n",
    "\n",
    "2. **For SALE model:**\n",
    "   - Load: `X_sale_train.npy`, `y_sale_train.npy`\n",
    "   - Train models (Linear Regression, Random Forest, XGBoost, etc.)\n",
    "   - Evaluate on `X_sale_test.npy`, `y_sale_test.npy`\n",
    "\n",
    "3. **For predictions on new data:**\n",
    "   - Load the appropriate preprocessor (`preprocessor_rent.joblib` or `preprocessor_sale.joblib`)\n",
    "   - Load target encodings\n",
    "   - Apply the same feature engineering\n",
    "   - Transform with the preprocessor\n",
    "   - Predict with your trained model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
